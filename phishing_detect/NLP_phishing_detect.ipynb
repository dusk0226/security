{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       text_combined  label\n",
      "0  endangered languages workshop foundation endan...      0\n",
      "1  claretta claretta_bordersfusemailcom cialis wo...      1\n",
      "2  roger upole schkeramsncom kyle rickey wrote im...      0\n",
      "3  barclays customer service testlightworldcojp d...      1\n",
      "4  gmm 09 nov 2001 please find attached global ma...      0\n",
      "                                      subject  \\\n",
      "0                   hpl nom for may 25 , 2001   \n",
      "1            re : nom / actual vols for 24 th   \n",
      "2  enron actuals for march 30 - april 1 , 201   \n",
      "3                   hpl nom for may 30 , 2001   \n",
      "4                   hpl nom for june 1 , 2001   \n",
      "\n",
      "                                                body  label  \n",
      "0  ( see attached file : hplno 525 . xls )\\r\\n- h...      0  \n",
      "1  - - - - - - - - - - - - - - - - - - - - - - fo...      0  \n",
      "2  estimated actuals\\r\\nmarch 30 , 2001\\r\\nno flo...      0  \n",
      "3  ( see attached file : hplno 530 . xls )\\r\\n- h...      0  \n",
      "4  ( see attached file : hplno 601 . xls )\\r\\n- h...      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Download stopwords if not already installed\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# Load phishing emails\n",
    "phishing_df = pd.read_csv(\"./dataset/phishing_email.csv\")\n",
    "\n",
    "# Balance amount of samples in two classes. \n",
    "phishing_df = phishing_df.sample(frac=0.4, random_state=42).reset_index(drop=True) \n",
    "\n",
    "# Load legitimate (ham) emails\n",
    "enron_df = pd.read_csv(\"./dataset/Enron.csv\")\n",
    "# Check data structure\n",
    "print(phishing_df.head())\n",
    "print(enron_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove HTML tags\n",
    "        text = re.sub(r\"<.*?>\", \"\", text)\n",
    "        # Remove special characters, digits\n",
    "        text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove stopwords\n",
    "        text = \" \".join(word for word in text.split() if word not in stop_words)\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "phishing_df[\"clean_text\"] = phishing_df[\"text_combined\"].apply(clean_text)\n",
    "enron_df[\"clean_text\"] = enron_df[\"body\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text  label\n",
      "0  nan mrs lisa williams recommended following pa...      1\n",
      "1  save prescription needs variety drugs reduced ...      0\n",
      "2  letter daniel kabila investment offer dear app...      0\n",
      "3  agree nominated totals eileen ponton david avi...      0\n",
      "4  email administratoradosashleyindonesiacom emai...      1\n",
      "                                              clean_text  label\n",
      "62756  spur site offer natural male enhancement formu...      0\n",
      "62757  dear vince delighted agreed take part energy p...      0\n",
      "62758  celebrate texas excellence ex students associa...      1\n",
      "62759  cnn alerts roxannakousebeetukanujatmx cnn aler...      1\n",
      "62760  automatically generated delivery status notifi...      0\n",
      "label\n",
      "1    32994\n",
      "0    29767\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "phishing_df[\"label\"] = 1  # Phishing\n",
    "enron_df[\"label\"] = 0  # Legitimate\n",
    "\n",
    "phish_df = phishing_df[[\"clean_text\",\"label\"]]\n",
    "legit_df = enron_df[[\"clean_text\",\"label\"]]\n",
    "# Merge datasets\n",
    "df = pd.concat([phish_df, legit_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the data\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.tail())\n",
    "\n",
    "# Check label distribution\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62761, 5000) <Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 155 stored elements and shape (2, 5000)>\n",
      "  Coords\tValues\n",
      "  (0, 2963)\t0.088585311705786\n",
      "  (0, 2937)\t0.16311008014946057\n",
      "  (0, 2597)\t0.16171642509399095\n",
      "  (0, 4905)\t0.2257999949946537\n",
      "  (0, 3686)\t0.0847953812764875\n",
      "  (0, 1746)\t0.049413012036330334\n",
      "  (0, 3188)\t0.125003725332856\n",
      "  (0, 165)\t0.062349633414917636\n",
      "  (0, 4861)\t0.06119788746580329\n",
      "  (0, 3366)\t0.030637537929715953\n",
      "  (0, 761)\t0.0487498165725552\n",
      "  (0, 2590)\t0.0581464979618392\n",
      "  (0, 4790)\t0.05776463638766167\n",
      "  (0, 1102)\t0.05254372802894687\n",
      "  (0, 1805)\t0.06986487722092984\n",
      "  (0, 2744)\t0.18894186263596932\n",
      "  (0, 4939)\t0.07201051909358007\n",
      "  (0, 392)\t0.1228657158591921\n",
      "  (0, 2638)\t0.06761040264069916\n",
      "  (0, 3020)\t0.1780344480548367\n",
      "  (0, 4979)\t0.10647202831370114\n",
      "  (0, 1207)\t0.14833836183058\n",
      "  (0, 4978)\t0.05182197516811045\n",
      "  (0, 4923)\t0.051374831780430855\n",
      "  (0, 711)\t0.1679699241954574\n",
      "  :\t:\n",
      "  (1, 3495)\t0.09132296071125191\n",
      "  (1, 4493)\t0.06721694498069199\n",
      "  (1, 4938)\t0.07636753797690526\n",
      "  (1, 1282)\t0.19903840690227725\n",
      "  (1, 4119)\t0.11427874520793015\n",
      "  (1, 1049)\t0.10260477871489979\n",
      "  (1, 4827)\t0.15226499770751153\n",
      "  (1, 1750)\t0.2900549704271767\n",
      "  (1, 59)\t0.11280276067731528\n",
      "  (1, 1673)\t0.0988390148271876\n",
      "  (1, 3105)\t0.05994779092669988\n",
      "  (1, 159)\t0.11733261679142153\n",
      "  (1, 4485)\t0.13256698288649604\n",
      "  (1, 1213)\t0.20387656603739762\n",
      "  (1, 4852)\t0.2554489243576043\n",
      "  (1, 4486)\t0.13711465166459744\n",
      "  (1, 1856)\t0.14292314637694056\n",
      "  (1, 4117)\t0.1167113069765824\n",
      "  (1, 3055)\t0.08225282053280307\n",
      "  (1, 77)\t0.1532734702288854\n",
      "  (1, 4179)\t0.13576991293689905\n",
      "  (1, 4651)\t0.08213468597670479\n",
      "  (1, 936)\t0.11543125076801347\n",
      "  (1, 1359)\t0.1612711898218407\n",
      "  (1, 3158)\t0.12237104413671568\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the words, keep only the top 5000.\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  \n",
    "X = vectorizer.fit_transform(df[\"clean_text\"]) \n",
    "print(X.shape,X[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "def train(model, X_train, y_train):\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "model_SVC = train(SVC(), X_train, y_train)\n",
    "y_pred_SCV = model_SVC.predict(X_test)\n",
    "accuracy_SVC = accuracy_score(y_test, y_pred_SCV)\n",
    "\n",
    "model_logistic = train(LogisticRegression(), X_train, y_train)\n",
    "y_pred_logistic = model_logistic.predict(X_test)\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of logistic regression model is 0.791444276268621. \n",
      " The accuracy of SVC model is 0.7857882577869832.\n"
     ]
    }
   ],
   "source": [
    "print(f'The accuracy of logistic regression model is {accuracy_logistic}.', \n",
    "      '\\n', f'The accuracy of SVC model is {accuracy_SVC}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
